{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec39309-3c50-4062-842d-e47c4cbd1139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Data, DataLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GCN, GAT\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalOutlierFactor\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCN, GAT\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94e67e-a6c2-4702-90b9-a979cc908ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = pd.read_csv('./data/edge_events.csv')\n",
    "# Extract unique nodes and create mapping dictionaries\n",
    "nodes = pd.unique(dataset[[\"src\", \"dst\"]].to_numpy().ravel())\n",
    "node2idx = {n: i for i, n in enumerate(nodes)}\n",
    "idx2node = {i: n for n, i in node2idx.items()}\n",
    "\n",
    "# Map source and destination nodes to integer IDs\n",
    "src = dataset[\"src\"].map(node2idx).to_numpy()\n",
    "dst = dataset[\"dst\"].map(node2idx).to_numpy()\n",
    "\n",
    "# Extract timestamps and labels\n",
    "t = dataset[\"timestamp\"].astype(int).to_numpy()\n",
    "msg = pd.get_dummies(dataset[\"label\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2ad7a-4c28-47b2-bd63-c88c791f7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph data\n",
    "df = pd.read_csv('graph_data.csv')\n",
    "adj = df.adj.toarray()\n",
    "x = df.x\n",
    "y = df.y\n",
    "\n",
    "# Define graph embedding model\n",
    "model = GCN(x, adj, num_layers=2, hidden_dim=64).to('cuda')\n",
    "\n",
    "# Define anomaly detection model\n",
    "X = torch.tensor(x)\n",
    "y_pred = np.zeros_like(x)\n",
    "scores = np.zeros_like(x)\n",
    "outlier_scores = np.zeros_like(x)\n",
    "\n",
    "for i, (data) in enumerate(DataLoader(Data(x=x, adj=adj, y=y), batch_size=len(x))):\n",
    "    out = model(data.x)\n",
    "    scores[i*len(x):(i+1)*len(x)] = out\n",
    "\n",
    "# Define anomaly detection algorithm\n",
    "lof = LocalOutlierFactor(n_neighbors=20, novelty=True)\n",
    "scores = lof.fit_predict(scores)\n",
    "\n",
    "# Normalize scores to [0, 1]\n",
    "scores = np.clip(scores, 0, 1)\n",
    "\n",
    "# Output anomaly scores\n",
    "anomaly_scores = pd.DataFrame({'node_id': np.arange(len(x)), 'anomaly_score': scores})\n",
    "\n",
    "# Define LLM interface\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def get_anomalies(question):\n",
    "    question = tokenizer.encode(question, return_tensors='pt')\n",
    "    inputs = {'input_ids': question['input_ids']}\n",
    "    outputs = model.generate(**inputs)\n",
    "    anomaly_scores = anomaly_scores.to_numpy().tolist()\n",
    "\n",
    "    return anomaly_scores\n",
    "\n",
    "# Example usage\n",
    "question = \"What are the top 5 most anomalous nodes in the graph?\"\n",
    "out = get_anomalies(question)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0b5f3-05c7-4397-b18b-0c80f3d61cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined_data)\n",
    "print(\"Dataset:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nTotal records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020a5f4-f028-43b1-a943-6562334eb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicKnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.node_to_idx = {}\n",
    "        self.idx_to_node = {}\n",
    "        self.edge_type_to_idx = {}\n",
    "        self.idx_to_edge_type = {}\n",
    "        self.node_features = {}\n",
    "        self.edges = []\n",
    "        self.timestamps = []\n",
    "        \n",
    "    def build_graph(self, df):\n",
    "        \"\"\"Build knowledge graph from DataFrame\"\"\"\n",
    "        node_idx = 0\n",
    "        edge_type_idx = 0\n",
    "        \n",
    "        # Create node mappings\n",
    "        all_nodes = set(df['src'].unique()) | set(df['dst'].unique())\n",
    "        for node in all_nodes:\n",
    "            if node not in self.node_to_idx:\n",
    "                self.node_to_idx[node] = node_idx\n",
    "                self.idx_to_node[node_idx] = node\n",
    "                node_idx += 1\n",
    "        \n",
    "        # Create edge type mappings\n",
    "        for edge_type in df['label'].unique():\n",
    "            if edge_type not in self.edge_type_to_idx:\n",
    "                self.edge_type_to_idx[edge_type] = edge_type_idx\n",
    "                self.idx_to_edge_type[edge_type_idx] = edge_type\n",
    "                edge_type_idx += 1\n",
    "        \n",
    "        # Build edges\n",
    "        for _, row in df.iterrows():\n",
    "            src_idx = self.node_to_idx[row['src']]\n",
    "            dst_idx = self.node_to_idx[row['dst']]\n",
    "            edge_type_idx = self.edge_type_to_idx[row['label']]\n",
    "            self.edges.append([src_idx, dst_idx])\n",
    "            self.timestamps.append(row['timestamp'])\n",
    "        \n",
    "        # Create node features (degree, edge type distribution, etc.)\n",
    "        self._create_node_features(df)\n",
    "        \n",
    "        return self._to_pyg_data()\n",
    "    \n",
    "    def _create_node_features(self, df):\n",
    "        \"\"\"Create node features based on graph statistics\"\"\"\n",
    "        # Initialize features for all nodes\n",
    "        for node in self.node_to_idx:\n",
    "            self.node_features[node] = {\n",
    "                'degree': 0,\n",
    "                'in_degree': 0,\n",
    "                'out_degree': 0,\n",
    "                'edge_types': defaultdict(int)\n",
    "            }\n",
    "        \n",
    "        # Calculate features\n",
    "        for _, row in df.iterrows():\n",
    "            src = row['src']\n",
    "            dst = row['dst']\n",
    "            edge_type = row['label']\n",
    "            \n",
    "            # Update degrees\n",
    "            self.node_features[src]['out_degree'] += 1\n",
    "            self.node_features[dst]['in_degree'] += 1\n",
    "            self.node_features[src]['degree'] += 1\n",
    "            self.node_features[dst]['degree'] += 1\n",
    "            \n",
    "            # Update edge type distribution\n",
    "            self.node_features[src]['edge_types'][edge_type] += 1\n",
    "            self.node_features[dst]['edge_types'][edge_type] += 1\n",
    "    \n",
    "    def _to_pyg_data(self):\n",
    "        \"\"\"Convert to PyTorch Geometric Data format\"\"\"\n",
    "        edge_index = torch.tensor(self.edges).t().contiguous()\n",
    "        \n",
    "        # Create feature matrix\n",
    "        num_nodes = len(self.node_to_idx)\n",
    "        num_edge_types = len(self.edge_type_to_idx)\n",
    "        \n",
    "        # Features: [degree, in_degree, out_degree, edge_type_features]\n",
    "        x = torch.zeros((num_nodes, 3 + num_edge_types))\n",
    "        \n",
    "        for node, idx in self.node_to_idx.items():\n",
    "            features = self.node_features[node]\n",
    "            x[idx, 0] = features['degree']\n",
    "            x[idx, 1] = features['in_degree']\n",
    "            x[idx, 2] = features['out_degree']\n",
    "            \n",
    "            # Edge type features\n",
    "            for edge_type, count in features['edge_types'].items():\n",
    "                edge_type_idx = self.edge_type_to_idx[edge_type]\n",
    "                x[idx, 3 + edge_type_idx] = count\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Build the knowledge graph\n",
    "kg = DynamicKnowledgeGraph()\n",
    "pyg_data = kg.build_graph(df)\n",
    "print(f\"Graph has {pyg_data.num_nodes} nodes and {pyg_data.num_edges} edges\")\n",
    "print(f\"Node features shape: {pyg_data.x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636e796-4737-4b0c-b90e-0df07c5a0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNAnomalyDetector(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNAnomalyDetector, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        # Reconstruction layer for anomaly scoring\n",
    "        self.reconstruction = nn.Linear(output_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # GNN layers\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = F.relu(self.conv2(h, edge_index))\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        h = F.relu(self.conv3(h, edge_index))\n",
    "        \n",
    "        # Reconstruction for anomaly detection\n",
    "        reconstructed = self.reconstruction(h)\n",
    "        \n",
    "        return h, reconstructed\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = pyg_data.x.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 16\n",
    "\n",
    "model = GNNAnomalyDetector(input_dim, hidden_dim, output_dim)\n",
    "print(\"GNN Anomaly Detector Model:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44a362-8fac-4225-b3dd-d6509910ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_detector(model, data, epochs=200):\n",
    "    \"\"\"Train the GNN anomaly detector in an unsupervised manner\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, reconstructed = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Reconstruction loss (mean squared error)\n",
    "        loss = F.mse_loss(reconstructed, data.x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def compute_anomaly_scores(model, data):\n",
    "    \"\"\"Compute anomaly scores for nodes based on reconstruction error\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings, reconstructed = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Compute reconstruction error for each node\n",
    "        reconstruction_error = F.mse_loss(reconstructed, data.x, reduction='none')\n",
    "        node_anomaly_scores = reconstruction_error.sum(dim=1)\n",
    "        \n",
    "        return node_anomaly_scores, embeddings\n",
    "\n",
    "# Train the model\n",
    "losses = train_anomaly_detector(model, pyg_data, epochs=200)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858b21f-1bc6-4fd9-97db-f53975a14555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores\n",
    "node_anomaly_scores, embeddings = compute_anomaly_scores(model, pyg_data)\n",
    "\n",
    "# Map scores back to node names\n",
    "anomaly_scores_dict = {}\n",
    "for node, idx in kg.node_to_idx.items():\n",
    "    anomaly_scores_dict[node] = node_anomaly_scores[idx].item()\n",
    "\n",
    "# Sort nodes by anomaly score (highest first)\n",
    "sorted_anomalies = sorted(anomaly_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top anomalies in the knowledge graph:\")\n",
    "print(\"-------------------------------------\")\n",
    "for i, (node, score) in enumerate(sorted_anomalies[:10]):\n",
    "    print(f\"{i+1}. Node: {node:<15} Anomaly Score: {score:.4f}\")\n",
    "\n",
    "# Visualize anomaly scores\n",
    "nodes = [item[0] for item in sorted_anomalies]\n",
    "scores = [item[1] for item in sorted_anomalies]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(nodes)), scores)\n",
    "plt.title('Node Anomaly Scores')\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.xticks(range(len(nodes)), nodes, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ae2e-94e4-4998-90e8-cac83a55ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAnomalyDetector:\n",
    "    def __init__(self, window_size=2):\n",
    "        self.window_size = window_size\n",
    "        self.temporal_patterns = defaultdict(list)\n",
    "        \n",
    "    def build_temporal_patterns(self, df):\n",
    "        \"\"\"Build temporal patterns for edge types between node pairs\"\"\"\n",
    "        # Group by timestamp and create graph snapshots\n",
    "        for timestamp in df['timestamp'].unique():\n",
    "            snapshot = df[df['timestamp'] == timestamp]\n",
    "            for _, row in snapshot.iterrows():\n",
    "                key = (row['src'], row['dst'], row['label'])\n",
    "                self.temporal_patterns[key].append(timestamp)\n",
    "                \n",
    "    def detect_temporal_anomalies(self, df):\n",
    "        \"\"\"Detect anomalies based on temporal patterns\"\"\"\n",
    "        anomaly_scores = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            key = (row['src'], row['dst'], row['label'])\n",
    "            timestamps = self.temporal_patterns[key]\n",
    "            \n",
    "            # Check if this edge appears at unusual times\n",
    "            current_time = row['timestamp']\n",
    "            \n",
    "            # If this edge type rarely occurs, it's anomalous\n",
    "            if len(timestamps) <= 1:\n",
    "                score = 1.0\n",
    "            else:\n",
    "                # Calculate temporal distribution anomaly\n",
    "                # Based on standard deviation of time intervals\n",
    "                intervals = [timestamps[i+1] - timestamps[i] \n",
    "                            for i in range(len(timestamps)-1)]\n",
    "                if intervals:\n",
    "                    mean_interval = np.mean(intervals)\n",
    "                    std_interval = np.std(intervals)\n",
    "                    # If current time deviates significantly from expected pattern\n",
    "                    if len(timestamps) > 1 and std_interval > 0:\n",
    "                        expected_next = timestamps[-1] + mean_interval\n",
    "                        temporal_deviation = abs(current_time - expected_next) / (std_interval + 1)\n",
    "                        score = min(temporal_deviation, 1.0)\n",
    "                    else:\n",
    "                        score = 0.1\n",
    "                else:\n",
    "                    score = 0.5\n",
    "                    \n",
    "            anomaly_scores.append(score)\n",
    "            \n",
    "        return anomaly_scores\n",
    "\n",
    "# Create temporal anomaly detector\n",
    "temporal_detector = TemporalAnomalyDetector()\n",
    "temporal_detector.build_temporal_patterns(df)\n",
    "\n",
    "# Compute temporal anomaly scores\n",
    "temporal_anomaly_scores = temporal_detector.detect_temporal_anomalies(df)\n",
    "\n",
    "# Add to dataframe\n",
    "df['temporal_anomaly_score'] = temporal_anomaly_scores\n",
    "\n",
    "print(\"Temporal anomaly detection:\")\n",
    "print(\"--------------------------\")\n",
    "# Show edges with highest temporal anomaly scores\n",
    "df_temporal_sorted = df.sort_values('temporal_anomaly_score', ascending=False)\n",
    "print(df_temporal_sorted.head(10)[['src', 'dst', 'label', 'timestamp', 'temporal_anomaly_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b933fe-4a0d-416e-9b65-adfa6478fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveAnomalyDetector:\n",
    "    def __init__(self, node_weight=0.4, edge_weight=0.3, temporal_weight=0.3):\n",
    "        self.node_weight = node_weight\n",
    "        self.edge_weight = edge_weight\n",
    "        self.temporal_weight = temporal_weight\n",
    "        self.kg = None\n",
    "        self.model = None\n",
    "        self.edge_detector = None\n",
    "        self.temporal_detector = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit the anomaly detector on the knowledge graph data\"\"\"\n",
    "        # Build knowledge graph\n",
    "        self.kg = DynamicKnowledgeGraph()\n",
    "        pyg_data = self.kg.build_graph(df)\n",
    "        \n",
    "        # Initialize and train GNN model\n",
    "        input_dim = pyg_data.x.shape[1]\n",
    "        hidden_dim = 32\n",
    "        output_dim = 16\n",
    "        \n",
    "        self.model = GNNAnomalyDetector(input_dim, hidden_dim, output_dim)\n",
    "        train_anomaly_detector(self.model, pyg_data, epochs=100)\n",
    "        \n",
    "        # Initialize edge detector\n",
    "        self.edge_detector = EdgeAnomalyDetector(self.kg)\n",
    "        \n",
    "        # Initialize temporal detector\n",
    "        self.temporal_detector = TemporalAnomalyDetector()\n",
    "        self.temporal_detector.build_temporal_patterns(df)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def detect_anomalies(self, df):\n",
    "        \"\"\"Detect anomalies in the knowledge graph\"\"\"\n",
    "        # Node anomaly scores\n",
    "        node_anomaly_scores, embeddings = compute_anomaly_scores(self.model, \n",
    "                                                                 self.kg._to_pyg_data())\n",
    "        node_scores_dict = {}\n",
    "        for node, idx in self.kg.node_to_idx.items():\n",
    "            node_scores_dict[node] = node_anomaly_scores[idx].item()\n",
    "            \n",
    "        # Edge anomaly scores\n",
    "        edge_anomaly_scores = self.edge_detector.compute_edge_anomaly_scores(df)\n",
    "        \n",
    "        # Temporal anomaly scores\n",
    "        temporal_anomaly_scores = self.temporal_detector.detect_temporal_anomalies(df)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = []\n",
    "        for i, row in df.iterrows():\n",
    "            src_score = node_scores_dict.get(row['src'], 0)\n",
    "            dst_score = node_scores_dict.get(row['dst'], 0)\n",
    "            edge_score = edge_anomaly_scores[i]\n",
    "            temporal_score = temporal_anomaly_scores[i]\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_score = (self.node_weight * (src_score + dst_score) / 2 + \n",
    "                            self.edge_weight * edge_score + \n",
    "                            self.temporal_weight * temporal_score)\n",
    "            combined_scores.append(combined_score)\n",
    "            \n",
    "        return combined_scores, node_scores_dict\n",
    "\n",
    "# Create and fit comprehensive anomaly detector\n",
    "comprehensive_detector = ComprehensiveAnomalyDetector()\n",
    "comprehensive_detector.fit(df)\n",
    "\n",
    "# Detect anomalies\n",
    "combined_scores, node_scores = comprehensive_detector.detect_anomalies(df)\n",
    "\n",
    "# Add combined scores to dataframe\n",
    "df['combined_anomaly_score'] = combined_scores\n",
    "\n",
    "print(\"Comprehensive anomaly detection results:\")\n",
    "print(\"=======================================\")\n",
    "print(\"Top anomalous edges:\")\n",
    "df_combined_sorted = df.sort_values('combined_anomaly_score', ascending=False)\n",
    "print(df_combined_sorted.head(10)[['src', 'dst', 'label', 'timestamp', 'combined_anomaly_score']])\n",
    "\n",
    "print(\"\\nNode anomaly scores:\")\n",
    "sorted_nodes = sorted(node_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (node, score) in enumerate(sorted_nodes[:10]):\n",
    "    print(f\"{i+1}. {node:<15} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20652262-3925-4e44-b7f9-9b818c1a0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "def detect_kg_anomalies(csv_data):\n",
    "    \"\"\"\n",
    "    Main function to detect anomalies in a knowledge graph from CSV data\n",
    "    \n",
    "    Parameters:\n",
    "    csv_data (str or DataFrame): CSV data with columns src, dst, label, timestamp, event_type\n",
    "    \n",
    "    Returns:\n",
    "    dict: Anomaly scores for nodes and edges\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle input data\n",
    "    if isinstance(csv_data, str):\n",
    "        # If string, assume it's CSV content\n",
    "        df = pd.read_csv(StringIO(csv_data))\n",
    "    else:\n",
    "        # If DataFrame, use directly\n",
    "        df = csv_data.copy()\n",
    "    \n",
    "    # Initialize and fit the comprehensive detector\n",
    "    detector = ComprehensiveAnomalyDetector()\n",
    "    detector.fit(df)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    combined_scores, node_scores = detector.detect_anomalies(df)\n",
    "    \n",
    "    # Add scores to dataframe\n",
    "    df['combined_anomaly_score'] = combined_scores\n",
    "    \n",
    "    # Results\n",
    "    results = {\n",
    "        'node_anomaly_scores': node_scores,\n",
    "        'edge_anomaly_scores': df[['src', 'dst', 'label', 'timestamp', 'combined_anomaly_score']].to_dict('records'),\n",
    "        'most_anomalous_nodes': sorted(node_scores.items(), key=lambda x: x[1], reverse=True)[:5],\n",
    "        'most_anomalous_edges': df.nlargest(5, 'combined_anomaly_score')[['src', 'dst', 'label', 'combined_anomaly_score']].to_dict('records')\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with the original data\n",
    "csv_content = \"\"\"src,dst,label,timestamp,event_type\n",
    "bng,trunk,HAS_PORT,0,add\n",
    "bng,trunk,HAS_PORT,0,add\n",
    "concentrator,trunk,DEPENDS_ON,0,add\n",
    "concentrator,trunk,DEPENDS_ON,0,add\n",
    "concentrator,port,HAS_PORT,0,add\n",
    "cpe,port,DEPENDS_ON,0,add\n",
    "cpe,trunk,DEPENDS_ON,0,add\n",
    "cpe,trunk,DEPENDS_ON,0,add\"\"\"\n",
    "\n",
    "test_df = pd.read_csv(StringIO(csv_content))\n",
    "anomaly_results = detect_kg_anomalies(test_df)\n",
    "\n",
    "print(\"ANOMALY DETECTION RESULTS SUMMARY\")\n",
    "print(\"=================================\")\n",
    "print(f\"\\nTop 3 Anomalous Nodes:\")\n",
    "for i, (node, score) in enumerate(anomaly_results['most_anomalous_nodes'][:3]):\n",
    "    print(f\"  {i+1}. {node}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 3 Anomalous Edges:\")\n",
    "for i, edge in enumerate(anomaly_results['most_anomalous_edges'][:3]):\n",
    "    print(f\"  {i+1}. {edge['src']} --[{edge['label']}]--> {edge['dst']} (Score: {edge['combined_anomaly_score']:.4f})\")\n",
    "\n",
    "print(\"\\n\\nThe system successfully detects anomalies in knowledge graphs using:\")\n",
    "print(\"- GNN-based node embedding and reconstruction\")\n",
    "print(\"- Edge pattern analysis for unusual relationships\")\n",
    "print(\"- Temporal analysis for timing irregularities\")\n",
    "print(\"- Weighted combination of all anomaly scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ada07e-448c-4367-a2a7-c64a93e5adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_combined_anomalies_fixed(df, node_scores):\n",
    "    \"\"\"Visualize the knowledge graph with combined anomaly insights (fixed version)\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add all nodes\n",
    "    for node in set(df['src'].unique()) | set(df['dst'].unique()):\n",
    "        G.add_node(node, node_score=node_scores.get(node, 0))\n",
    "    \n",
    "    # Create edge keys to avoid duplicate edges in NetworkX\n",
    "    edge_data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        edge_key = (row['src'], row['dst'])\n",
    "        if edge_key not in edge_data:\n",
    "            edge_data[edge_key] = {\n",
    "                'labels': [],\n",
    "                'scores': [],\n",
    "                'combined_scores': []\n",
    "            }\n",
    "        edge_data[edge_key]['labels'].append(row['label'])\n",
    "        edge_data[edge_key]['scores'].append(row['edge_anomaly_score'])\n",
    "        edge_data[edge_key]['combined_scores'].append(row['combined_anomaly_score'])\n",
    "    \n",
    "    # Add edges to graph (average scores for duplicate edges)\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    edge_labels = {}\n",
    "    \n",
    "    for (src, dst), data in edge_data.items():\n",
    "        G.add_edge(src, dst)\n",
    "        # Average scores for multiple edges between same nodes\n",
    "        avg_edge_score = np.mean(data['scores'])\n",
    "        avg_combined_score = np.mean(data['combined_scores'])\n",
    "        edge_colors.append(avg_combined_score)\n",
    "        edge_widths.append(max(1, avg_combined_score))\n",
    "        # Combine labels for visualization\n",
    "        edge_labels[(src, dst)] = '/'.join(data['labels'])\n",
    "    \n",
    "    # Node sizes based on node anomaly scores\n",
    "    node_sizes = [max(300, node_scores.get(node, 0) * 50) for node in G.nodes()]\n",
    "    node_colors = [node_scores.get(node, 0) for node in G.nodes()]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                                   node_color=node_colors, cmap=plt.cm.Reds, alpha=0.8)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, \n",
    "                           edge_cmap=plt.cm.Blues, width=edge_widths, alpha=0.6, \n",
    "                           arrowstyle='->', arrowsize=20)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "    \n",
    "    # Draw edge labels\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)\n",
    "    \n",
    "    # Add colorbars\n",
    "    plt.colorbar(nodes, label='Node Anomaly Score', shrink=0.8)\n",
    "    \n",
    "    plt.title('Knowledge Graph Anomaly Detection\\nNode size/color = Node anomaly score, Edge width/color = Edge anomaly score')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return G, pos\n",
    "\n",
    "# Create final visualization\n",
    "G, pos = visualize_combined_anomalies_fixed(df, node_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
